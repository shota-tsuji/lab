## 椋木さんのA study on linear algebra operations using extended precision floating-point arithmetic on GPUs
- Utilizing the quadruple-precision floating-point arithmetic operation for the Krylov SubspaceMethods
- Extended-Precision Floating-Point Numbers for GPU Computaion[26]
- Scalar Fused Multiply-Add Instructions Produce Floating-Point Matrix Arithmetic Provably Accurate to the Penultimate Digit
- A Control Method of Errors in Long-Term Integration
- Fast Implementation of DGEMM on Fermi GPU
- An Improved MAGMA GEMM for Fermi GPUs
- A Fast GEMM Implementation On the Cypress GPU
- A Fast Implementation of Matrix-matrix Product in Double-double Precision on NVIDIA C2050 and Application to Semidefinite Programming
- XBLAS – Extra Precise Basic Linear Algebra Subroutines
- The International Exascale Software Project: a Call To Cooperative Action By the Global High-Performance Community
- Efficient Sparse Matrix-Vector Multiplication on CUDA
- Optimizing Sparse Matrix Vector Multiplication Using Cache Blocking Method on Fermi GPU
- [Optimization of Sparse Matrix-Vector Multiplication with Variant CSR on GPUs](#optimization-techniques-for-sparse-matrix–vector-multiplication-on-gpus)
- Accelerating Sparse Matrix Vector Multiplication in Iterative Methods Using GPU
- Optimization of Sparse Matrix-Vector Multiplication by Auto Selecting Storage Schemes on GPU
- Optimizing Sparse Matrix-Vector Multiplication on GPUs, IBM Research Report
- Auto-Tuning CUDA Parameters for Sparse Matrix-Vector Multiplication on GPUs
- Generating Optimal CUDA Sparse Matrix Vector Product Implementations for Evolving GPU Hardware[51]
- Efficient sparse matrix-vector multiplication on cache-based GPUs
- Automatic Tuning of Sparse Matrix-Vector Multiplication for CRS format on GPUs
- Efficient sparse matrix-vector multiplication on cache-based GPUs
- Automatic Tuning of Sparse Matrix-Vector Multiplication for CRS format on GPUs
- A Memory-Bound Application on the GPU Stuck Between a Rock and a Hard Place
- Templates for the Solution of Linear Systems: Building Blocks for Iterative methods, 2nd Edition
- ccelerating scientific computations with mixed precision algorithms
- Implementation of Fast Quad Precision Operation and Acceleration with SSE2 for Iterative Solver Library
- Development of a Stokes flow solver robust to large viscosity jumps using a Schur complement approach with mixed precision arithmetic
- Solving finite difference linear systems on GPUs: CUDA based Parallel Explicit Preconditioned Biconjugate Conjugate Gradient type Methods[#]
- Incomplete-LU and Cholesky Preconditioned Iterative Methods Using CUSPARSE and CUBLAS
- Fast Quadruple Precision Arithmetic Library on Parallel Computer SR11000/J2
- Autotuning GEMM Kernels for the Fermi GPU[70]

## Extended-Precision Floating-Point Numbers for GPU Computaion[26] 
- High-precision floating-point arithmetic in scientific computation
- Performance and accuracy of hardware-oriented native-, emulated- and mixed-precision solvers in FEM simulations
- A floating point technique for extending the available precision
- A FORTRAN multiple-precision arithmetic package
- A Fortran package for floating-point multiple-precision arithmetic
- A Fortran 90-based multiprecision system
- Adaptive precision floating-point arithmetic and fast robust geometric predicates
- A Fortran-90 double-double precision library
- Algorithms for quad- double precision floating point arithmetic
- Quad-double arithmetic: algorithms, implementation, and application
- The GAIA Project: evaluation of GPU-based programming environments for knowledge discovery
- Extended-precision floating-point numbers for GPU computation[21]
- A new range-reduction algorithm
- Some functions computable with a fused-mac, arith

## Fast Implementation of DGEMM on Fermi GPU
- Self-adapting linear algebra algorithms and software
- Anatomy of high-performance matrix multiplication
- A note on auto-tuning gemm for gpus


## Efficient Sparse Matrix-Vector Multiplication on CUDA[*, 2008]
2. Solving dense linear systems on graphics processors[2008]
3. Prefix sums and their applications[1990]
4. Segmented operations for sparse matrix computation on vector multiprocessors[1993]
5. Concurrent number cruncher - a GPU implementation of a general sparse linear solver
6. Vectorized sparse matrix multiply for compressed row storage format[2006]
8. Sparsity: Optimization framework for sparse matrix kernels[2004]
9. Toward the optimal preconditioned eigensolver: Locally optimal block preconditioned conjugate gradient method[2002]
10.  Basic Linear Algebra Subprograms for Fortran Usage[1979]
11. Approximation algorithms for scheduling unrelated parallel machines[1990]
13. Scalable parallel programming with CUDA[2008]
16. Iterative Methods for Sparse Linear Systems[2003]
19. Optimization of sparse matrix-vector multiplication on emerging multicore platforms[2007]
### cited
- Implementing Sparse Matrix-Vector Multiplication on Throughput-Oriented Processors[2009]
- Model-driven autotuning of sparse matrix-vector multiply on GPUs[2010]
- State-of-the-art in heterogeneous computing[2010]
- Optimizing sparse matrix-vector multiplication on GPUs using compile-time and run-time strategies[2008]
-

## Optimization techniques for sparse matrix vector multiplication on GPUs[*]
- The landscape of parallel computing research: A view from Berkeley
- Fast sparse matrix–vector multiplication on GPUs for graph applications
- An efficient two-dimensional blocking strategy for sparse matrix–vector multiplication on GPUs
- Warps and atomics: Beyond barrier synchronization in the verification of GPU kernels
- Optimizing Sparse Matrix-Vector Multiplication on GPUs
- Implementing sparse matrix–vector multiplication on throughput-oriented processors
- Matrix market: A web resource for test matrix collections
- Model-driven autotuning of sparse matrix–vector multiply on GPUs
- SpMV: A memory-bound application on the GPU Stuck between a Rock and a Hard Place
- Optimization of sparse matrix–vector multiplication with variant CSR on GPUs
- High-performance graph algorithms from parallel sparse matrices
- Efficient sparse matrix–vector multiplication on GPUs using the CSR storage format
- Adaptive row-grouped CSR format for storing of sparse matrices on GPU
- Compressed multirow storage format for sparse matrices on graphics processing units
- A unified sparse matrix data format for modern processors with wide SIMD units
- Evaluation criteria for sparse matrix storage formats
- AdELL: An adaptive warp-balancing ELL format for efficient sparse matrix–vector multiplication on GPUs
- CoAdELL: Adaptivity and compression for improving sparse matrix–vector multiplication on GPUs
- GPU-based steady-state solution of the chemical master equation
- Automatically tuning sparse matrix–vector multiplication for GPU architectures
- New row-grouped CSR format for storing the sparse matrices on GPU with implementation in CUDA
- Optimization of sparse matrix–vector multiplication using reordering techniques on GPUs
- Iterative Methods for Sparse Linear Systems
- Accelerating sparse matrix–vector multiplication on GPUs Using bit-representation-optimized schemes
- A new approach for sparse matrix vector product on NVIDIA GPUs
- Improving the performance of the sparse matrix vector product with GPUs
- Roofline: An insightful visual performance model for multicore architectures
- Sparse matrix–vector multiplication optimizations based on matrix bandwidth reduction using NVIDIA CUDA
- yaSpVM: Yet another SpMV framework on GPUs


## Accelerating Sparse Matrix Vector Multiplication in Iterative Methods Using GPU
- Optimizing sparse matrix-vector multiplication on gpus
- Implementing sparse matrix-vector multiplication on throughput-oriented processors
- Model-driven autotuning of sparse matrix-vector multiply on GPUs
- Acceleration of conjugate gradient method for circuit simulation using CUDA
- Implementing Blocked Sparse Matrix-Vector Multiplication on NVIDIA GPUs
- Automatically tuning sparse matrix-vector multiplication for GPU Architectures
- Designing efficient sorting algorithms for manycore GPUs
- Efficient Breadth-First Search on the Cell/BE Processor
- Accelerating Large Graph Al-gorithms on the GPU Using CUDA
- Improving the Performance of the Sparse Matrix Vector Product with GPUs
- Optimization of sparse matrix-vector multiplication on emerging multicore platforms
- Fast
- Sparse Matrix-Vector Multiplication on GPUs: Implications for Graph Mining
- Optimization of linked list prefix computations on multithreaded GPUs using CUDA

## Optimization of Sparse Matrix-Vector Multiplication by Auto Selecting Storage Schemes on GPU(2011)[*]
### notes
- download pdf is book, "Springer 2011", so large, not downloaded
- Efficient Sparse Matrix-Vector Multiplication on CUDA
- Aegmented Operations for Sparse Matrix Computation on Vector Multiprocessors
- High performance conjugate gradient solver on multi-gpu clusters using hypergraph partitioning
- SPARSKIT: a basic tool kit for sparse matrix computations - Version 2 
### cited
- Performance analysis and optimization for SpMV on GPU using probabilistic modeling[2015]
- Optimization of sparse matrix-vector multiplication for CRS format on NVIDIA Kepler architecture GPUs[2013]
- Automatic tuning of sparse matrix-vector multiplication for CRS format on GPUs[2012]
- Accurate cross‒architecture performance modeling for sparse matrix‒vector multiplication (SpMV) on GPUs[2015]
- Sparse matrix-vector multiplication on GPGPUs[2017]
- The DynB Sparse Matrix Format Using Variable Sized 2D Blocks for Efficient Sparse Matrix Vector Multiplications with General Matrix Structures[2017]
- Multi-GPU implementation and performance optimization for CSR-based sparse matrix-vector multiplication[2017]
- Performance Optimization for SpMV on Multi-GPU Systems Using Threads and Multiple Streams[2016]

## Performance analysis and optimization for SpMV on GPU using probabilistic modeling(not read)
- Implementing Sparse Matrix-Vector Multiplication on Throughput-Oriented Processors
- Optimization of Quasi Diagonal Matrix-Vector Multiplication on GPU
- Sparse Matrix Solvers on the GPU: Conjugate Gradients and Multigrid
- Improving the Performance of the Sparse Matrix Vector Product with GPUs
- Automatically Generating and Tuning GPU Code for Sparse Matrix-Vector Multiplication from a High-Level Representation
- Optimization of Sparse Matrix-Vector Multiplication Using Reordering Techniques on GPUs[*]
- New Row-Grouped CSR Format for Storing the Sparse Matrices on GPU with Implementation in CUDA
- High-Level Strategies for Parallel Shared-Memory Sparse Matrix-Vector Multiplication(2014)
- Model-Driven Autotuning of Sparse Matrix-Vector Multiply on GPUs
- An Extended Compression Format for the Optimization of Sparse Matrix-Vector Multiplication
- Optimizing Matrix Multiplication for a Short-Vector SIMD Architecture-Cell Processor
- Optimizing Sparse Matrix-Vector Multiplication on GPUs
- Self-Adapting Linear Algebra Algorithms and Software
18. Optimizing Sparse Matrix-Vector Multiplication on CUDA
19. Fast Sparse Matrix-Vector Multiplication on GPUs: Implications for Graph Mining
20. Generating Optimal CUDA Sparse Matrix-Vector Product Implementations for Evolving GPU Hardware
24. Performance of a Structure-Detecting SpMV Using the CSR Matrix Representation
25. Parallel Sparse Approximate Inverse Preconditioning on Graphic Processing Units
27. An Analytical Model for a GPU Architecture with Memory-Level and Thread-Level Parallelism Awareness
29. SMAT: An Input Adaptive Auto-Tuner for Sparse Matrix-Vector Multiplication
30. Exploring the Multiple-GPU Design Space
33. Architecture- and Workload-Aware Heterogeneous Algorithms for Sparse Matrix Vector Multiplication

いちばん（自分にとって）効率のよい方法とは
一辺倒になりすぎるのではなく、中間的な位置でつねに場合によって考えるスタンスでいる。

気になった点
目的の2項目は、レベルさげる
*単語の区切りで、改行(p8など)
p13 計算環境は表にするとわかりやすい
p15 非ゼロ要素数(小→大)
p?? 実行時性能がことなるのはなしは、書いておく必要あり？
p18 格納形式でグラフ2つにわけると、gpuアーキテクチャ間での比較が見やすい

spmvはcusparseにおいて、様々ん格納形式において実装されている

句点で、読みの息継ぎすると、頭の中で休憩しながら読めるかも

研究室のhow-to本とか読むと、細かいところと大切なところがわかって、自己流をさけると、効率上がる


有限要素法no(櫻井先生)
祖業エレツべく売ろつ
複数ベクトル　有限要素の代表的な行列にかぎって
auto-tuning
v100でどうするかは
対等な関係だとよい
公費でかってもらうのが、必ずしもよいとは限らない
すぐに陳腐化する本なら、研究費で買うとよい
自分のそばにおいておきたい本は、自分で買う（ブルーバックスなど安いものは自分でも買える）

備品リストには、場所かく（なにがどこにあるかを把握するのが目的）
