## overview
ページ数 : 9

contents
- 2.7 : Single-instruction, multiple data array
	- 2.7.1 : Single-instruction, multiple data architecture
	- 内部機能としての要素がたくさん
	- 2.7.2 : Amdahl's law
- 2.8 : Multiprocessors
	- 文章たくさん, 今後の説明
	- 2.8.1 Shared-memory multiprocessors
	- numaとか
	- 2.8.2 Massively parallel processors
	- やはり図とか
	- 2.8.3 Commodity clusters(商品・既成品としてのクラスタ)

## 2.7
SIMDを使った配列（処理）は1980~1990年代の並列処理をするコンピュータのアーキテクチャとしてメジャーな（類の）分野だった．
大規模集積回路の技術にとくにマッチした．
SIMDのような処理を行うシステムは，その世代の前後でも作られていたわけだが．
//並列処理におけるこの手法は，特定の処理と特定のアクセラレータのための最近のコンピュータにおいて幅広く要素技術として見受けられる.
並列処理のためのこの手法は，幅広いコンピュータの要素技術として見受けられる-特定の仕事と高速化のために
### 2.7.1
SIMD配列の類の並列コンピュータアーキテクチャは非常に多数の比較的単純なプロセスエレメントから成るものだった．
それぞれプロセスは，各自のもつデータメモリに対して操作をおこなっていた．
プロセスは，すべてのプロセスに順番に命令をブロードキャストするようにしてコントロールされていた．shared sequencer または sequence controllerと呼ばれるものによって
処理時間内においては，つねに，すべてのプロセスが同じ処理を与えられたメモリブロック（一部）に対して行うかたちをとっていた．

SIMD配列アーキテクチャは，独立したシステム or	アクセラレータとして他のコンピュータシステムとともに組み込まれることで，用いられていた．

SIMD配列としてのプロセスは，このレベル（どのレベル？）における並列性を通して，潜在的に非常に高い性能を獲得するために,たくさん複製された．
標準的なプロセスは，これから挙げるような，重要な（キーとなる）内部機能要素から構成されている．

Memory block
- 直接アクセスできるシステム全体のメモリの一部分を，個々のプロセスに提供する（機能）

ALU
- ローカルメモリ上のデータ内容を扱う操作を実行する
- 大抵は，sequence controllerからブロードキャストされた命令に含まれる，追加の（即値オペランドとしての）即値とともにローカルのレジスタを通して

Local egisters
- プロセスで実行される操作のために使われている値を格納する．
- load/storeアーキテクチャでは，ローカルメモリへの直接的なインターフェースとなっている．
- ローカルレジスタは，（システムに広がるネットワーク上や他のリモートのプロセスから取ってくる)ローカルにないデータの通信を行うための中間バッファ（仲介役?）として動作する
- I/O channelsと同じように

Sequencer controller
- system instruction sequencerから流れてくる命令を受け付ける
- それぞれの命令をデコードする
- 必要になったローカルプロセスをコントロールする信号を作りだす(マイクロ命令の並びとして)
- microoperations : 命令セットアーキテクチャによって定められた命令を，プロセッサ内でさらに分割した単純な命令のこと

Instruction interface
-	sequence controllerから流れてくる命令列を分配する，broadcast networkにつながったポート

Data interface
- プロセスがもつmemory block間のデータを交換するためのシステム上のデータネットワークにつながったポート

External I/O interface
- 個々のPEを関連づけるシステムのためのポート
- システムの外部I/Oチャンネルがついている
- PEは与えられたポートに対するdirect interfaceを含んでいる

SIMD配列のsequence controllerは，まとまったプロセスによって実行される操作を定義している
またSIMD配列のsequence controllerは計算作業そのもののうち，いくつかを担っている．
sequence controllerは多様な形態をとることがあり，今日においてもそれ自体が新しい設計のためのターゲットとなっている．
しかし一般的な感覚としては，一連の機能と構成要素がほとんどのばらつき（変化）を統一している．

第一近似（一次近似）としてAmdahlの法則が使われるだろう-古典的なSIMD arrayコンピュータにおけるパフォーマンスの向上を評価するために
仮定する-与えられたひとつの命令サイクルにおいて，すべてのarray processor cores(p_n)がそれぞれの操作を同時に行う場合-or-controll sequenceが逐次命令を行う（array processor coresは休んだまま）場合
また仮定する-サイクル数fはarray processor coresを利用できるとする
このとき，Amdahlの法則を使って，speed up(S)は次のように決まる．
## 2.7.2
理想的には，はじめからおわりまですべての計算部分が，同時に実行されるような並列な部品に分割できるはず，
- たくさんの計算資源が計算に同時に利用される
	- 計算の中で（間）は，解を得る（終了）までの時間を均一する
	- 処理速度を上げる
この理想的なケースのように並列化できる極端な例も存在するが，大抵のアプリケーション・プログラムでは確かに並列に計算する部分に加えて，（限界まで並列化しても）並列部分が少ない or 全く並列化されない部分が現れる．
	- 可能な部分 : 同時計算を通して加速するような
	- 並列化されない部分 : そのまま逐次で実行して，一度に一命令を発行するような
	
?computing profile?
全体の実行の中で並列化部分と逐次部分の両方を組み合わせて持っている，この計算の特徴（に関する解析）は，並列化を通して達成される最大の高速化において，重要なせんびきを行う（境界線を引く）
この境界条件の最も広く知られている公式化はAmdahlの法則として参照される-それを初めて成文化した有名なコンピュータアーキテクトにちなんで
一般的に並列計算へより広範囲に（わたって）適用可能である一方，Amdahlの法則は特にSIMD array計算の性能のモデリングによくあっている．（いくつかのわずかな単純化の前提とともに）-そして，これがここでのAmdahlの法則の導入の動機づけている．
のちほど，それは，並列アーキテクチャの他の形式を理解するために使われる．
SIMDは2つの実行の形式をもつことを想定する-sequential-中央演算装置が一度に一命令を実行する-or-parallel-すべてのarray processor coresが同時にそれぞれの操作を実行するような
簡単のため，中央演算装置とarray porcessor coresの両方のクロックレートは同じである前提をおく-しかしこれはほとんど重要ではない-このパフォーマンスモデルにとっては
fig2.14 and 2.15は2つのタイムラインを示している-ひとつめはすべての計算操作の逐次実行T_0-ふたつめは操作fの割合（部分）, T_f-並列で行われ，並列のレベルgで
理想的には，パフォーマンスの向上はgとなる．
しかし，少なくともある程度並列化して実行されるのは，全体T0操作のうちT_fのめである．-T0-T_fの操作はまだ逐次的に実行される．- f=Tf/T0
以下に示すように，実際のスピードアップsは定まる-2つの回答時間（g-並列化したものとしていないもの）によって
s and Ta
結果的なsの公式化は以下のようになる-gとfの関数として-関連する正確な時間とはどくりつして
変数定義（少しむづかしいのだけめも）
TF : 高速化可能な計算の割合の時間
g : 最大のパフォーマンス向上-計算の高速化された割合のための？
f : 高速化していないものの分数-高速化したぶんの割合
s : 高速化を適用したときの計算のスピードアップ

このAmdahlの法則の公式化は，様々な可変な動作点を考慮することで理解できる．
限界（上界？）として，すべてのコードを並列性gで等しく実行したとすると，分数f=1となり，全体としてのスピードアップは理想的な場合としてgとなる．
しかし，コードに並列化できる箇所がない場合，並列度gでのハードウェア並列化をもっているにもかかわらず，f=0,s=1となり，予想通り向上は見られない．
What is more sobering, and なぜAmdahlの法則は重要なのかということは，さらに別の動作点を見ていこう．
並列化するハードウェアが1million(g=1,000,000)の理想的な向上を示している（可能であるてきな意味）ことと，コードの半分が同時実行できること(f=0.5)を仮定する．
単純に代入すると，莫大な潜在的な向上にもかかわらず，実際にもたらすスピードアップは2未満となる．
実際には，gが無限であっても，f=0.5であればスピードアップは2より大きくはならない．
スピードアップの範囲がfig2.16に示されている-全体時間に対する高速化部分の割合に関して-ことなる理想的な高速化による向上(accelerator gains)

例
SIMD array computerを考える
- コアプロセッサの8 * 8 array
- ひとつのsequential controll processor
与えられた計算サイクルで
- control processor がひとつの操作を実行する
- all the array coresがそれぞれのデータに対して同じ操作を実行する
全体の仕事量のうちどれくらいが必要となるか-core arrayを用いて全体として8倍のスピードアップを図るためには
s=8 にするために，g=64のときの高速化による向上であるときに
適切に代入

ここでは示されている
- 64並列による最大高速化のうちたった12.5%(12.5 = 8 / 64)を達成するためには，仕事量の90%が並列化可能であることが条件となる．
このような計算過程から，これ（8倍にすること）が無理難題で達成困難であることがわかる．

## Multiprocessors
並列計算機のマルチプロセッサは，今日のスーパーコンピュータの支配的な形態（多くをしめる）
もっとも広く捉えると，1つの仕事を実行するためにコミュニケーションネットワークで統合・調整された個々の自立しているコンピュータの集合を含むすべてのシステム
Flynn taxonomyによれば，マルチプロセッサはMIMDクラスのマシンである
システムを作り上げているそれぞれのプロセッサは，ローカルなinstruction stream processorが制御しているデータ処理ユニットを持っている
マルチプロセッサは，1950年代やSAGEまで遡る長い歴史を有している
- SAGE : 北米航空宇宙防衛司令部にあるUS Air Forceのために，IBMが配備したMITのWhirlwind-class computersからなる
多くの商用マルチプロセッサが配備された
- 各マルチプロセッサは2つのプロセッサを含む
	- ひとつのプロセッサは計算に使われる(重労働)(計算用)
	- 他方はI/O処理を管理用
- マルチプロセッサはスーパーコンピュータの重要性で成長した
	- VLSI(very large scale integration)技術の出現とマイクロプロセッサアーキテクチャの開発とともに
これは重要な変化を代表していた
	- スーパーコンピュータアーキテクチャの傾向と方向における
?費用利益が配備コンピュータの次世代を決めた
- 一般用途のためのマルチプロセッサのmass marketに由来する
- 規模の経済の搾取

ワークステーション,個人用コンピュータ,企業用サーバといったより広い市場に派生したマルチプロセッサの統合は
- スーパーコンピュータの主要な計算エンジンのように
- 大規模なシステムアーキテクチャに劇的な影響を与えた
- 以前の特定のデザインを大きく置き換えていきながら
- スーパーコンピュータの主要な計算エンジンが大規模なシステムアーキテクチャに劇的な影響を与えるにつれて
目に見える影響として，スーパーコンピュータの純粋な物理的なサイズが挙げられる
- 段々と，多くの VLSIマイクロプロセッサを組み込んだラックの列が拡大していった
今日では，マイクロプロセッサは技術的にさらに別の飛躍をとげている
- それぞれのソケットがマルチプロセッサを組み込んでいるようなマルチコア技術のここ10年間のマルチコア技術
- "cores"と呼ばれる

3つの主要な構成が用いられている
- SMPs
- MPPS
- commodity clusters
ひとつのプロセッサシステムは統一されたメモリ(unified memory)を反映している
- すべてのデータが同じメモリのサブシステムに収まっている
複数のプロセッサが動くときには，プロセッサとメモリの相互関係の方法について選択肢が与えられる
- システム内にあるすべてのプロセッサが同じメモリのサブシステムを共有する
- それぞれのプロセッサが独立したメモリをもつか
- 第3の選択肢としてその間をとったもの
	- プロセッサのグループがメモリブロックを共有し，グループごとに異なるメモリブロックもつ
	- 他のグループは"nodes"と呼ばれる
これ（第3の選択肢）は，マルチコアソケットとともによく用いられる構造である
これらの異なるmultiprocessor system architectureについては，続くサブセクションで詳細を述べる

## 2.8.1 Shared-Memory Multiprocessors
共有メモリマルチプロセッサは，適度な数のプロセッサからなるアーキテクチャである
- すべてのプロセッサがシステムのメインメモリに直接アクセスする（ハードウェア的に）
- すべてのシステムプロセッサがデータにアクセスできる
	-	他のプロセッサが作ったデータ
	-	これから使うであろうデータ
この形態のマルチプロセッサアーキテクチャの鍵は，相互接続ネットワークである
- すべてのプロセッサをメモリにつなげている
- システム上のすべてのプロセッサのもつキャッシュにまたがって，キャッシュの一貫性を保つ必要があるので，複雑である


キャッシュの一貫性は確実にする
- キャッシュのどのデータ変更も反映されていること
- ?同じグローバルデータの位置をもつの他のすべてのキャッシュへ変更を加えることにより
- これによりプロセッサレジスタへの(任意の)データのロード・ストアが正しいことが保証される
	- ローカルキャッシュから取ってくる場合でも
	- 他のプロセッサが同じデータを使っていたとしても
相互接続ネットワーク(キャッシュ一貫性を提供するもの)は，いくつかの技術のうちひとつを用いている
- modified exclusive shared invalid(MESI) protocol 
	- snooping cacheとも呼ばれる
	- 共有バスを使って，すべてのプロセッサとメモリをつなげている
	- この方式では，他のすべてのプロセッサはあるプロセッサのメモリへの書き込みを検出できる
	- 同じ記憶領域がローカルにキャッシュされているか確認できる
	- そうであれば，印を記録して，キャッシュを更新or無効化する
	- エラーが起こらないように

共有メモリマルチプロセッサは差別化されている
- プロセッサが共有メモリブロックにアクセスする相対時間によって
- SMPはシステムアーキテクチャ
	-	すべてのプロセッサが，同じ時間でそれぞれのメモリブロックへアクセスできる
		- これはuniform memory access,"UMA"と呼ばれる
	- SMPsは，すべてのプロセッサとネットワークに渡る1つのOSが管理している
		- 複数のメモリバンクへの直接的なアクセスを提供するバスまたはクロスバーのようなネットワーク
- アクセス時間には，まだばらつきがある
	- 競合がおこると，1つ以上の	プロセッサのアクセス時間が遅延される
	- ひとつのメモリバンクへの2プロセッサ以上でのアクセス
- しかしながら，すべてのプロセッサは同等の機会と等しいアクセスができる
- 早期のSMPs
	- 1980年代に現れた
	- 同様のシステム
	- Sequent Balance 8000として
- 今日のSMPs
	-	企業のサーバ
	-	デスクトップ
	-	マルチコアチップのノートパソコンとしても
	-	そのため，商業市場において主要である中規模計算において重要な役割を果たしている
	-	SMPsは，遥かに大きいMPPsの中のnodesとしても動いている
	
Nonuniform memory access(NUMA)アーキテクチャ
- すべてのプロセッサが任意のメインメモリブロックへアクセスできる
- しかし，すべてのメモリブロックへのアクセス時間が等しいことは保証していない
	- これはモダンなマイクロプロセッサデザインによる? the architecture opportunityに動機づけられる
	- 高速なlocal memory communication channelsを活用するために
	- 一方ですべてのメモリへもアクセスできる
		- 外付けで遅い，グローバルな相互接続ネットワークを通して
NUMAアーキテクチャの利点はscaling
- さらに多くのプロセッサコアをひとつの共有メモリシステムに組み込む場合
- SMPsよりも多く
NUMAの注意点
- メモリアクセス時間の差異により
	- データ配置の局所性を意識する必要あり
	- 計算資源をフル活用する必要あり
NUMAマルチプロセッサアーキテクチャの出現
- BBN Butterfly multiprocessors
- GP-1000 and TC-2000を含んでいた

## 2.8.2 massively parallel processors
MPPアーキテクチャ
- 計算システムのサイズとパフォーマンスをスケールするのがいちばん簡単
- 今日の最大規模(processor coresが1,000,000)のスーパーコンピュータもこのアーキテクチャ
- 分散メモリアーキテクチャ

- processor coresのグループごとに，ローカルメモリへ直接繋がっている
	- nodesと呼ばれる
	- node間のメモリ共有はない
		- これにより，設計の簡易化とスケーラビリティを妨げる非効率な部分の削除
	- 共有メモリがないことによるデメリット
	- 他のグループとのデータ交換に，別の方法が必要
	- グループ間での調整も必要

- message passingのための論理機能に関して
	- 物理的なsystem area network(SAN)がによって可能に
		- ひとつのシステムを形成するために，すべてのノードを統合
	- システム中の2つのprocessor cores間でmessageをやりとり
		- コアごとに別々のプロセスを走らせている
		- 詳しくはChapter 8
		- この方法により，受け取りプロセスとhost processorは，送信側からデータを取得できる
	
	- このネットワークを用いて，別々のprocessorで走るプロセスの同期をとれる

- Intel ASCI Red MPP
	- the first system capable of teraflops
	- By 1997
	- HPL benchmark
	- deployed at Sandia National Laboratories
